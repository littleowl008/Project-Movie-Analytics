# -*- coding: utf-8 -*-
"""Project Movie Analytics.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1i3V-4MF7kbbXQ3TSWGO0LTc0-Hp7pwVu

IMPORT DATA

https://drive.google.com/file/d/1cy6jfvzx598rYHGOdkx-c4pzUx7MqCub/view?usp=drive_link
https://drive.google.com/file/d/1c6rHCCpF3GFW9KO7P0JTF9ilKhbYGqJB/view?usp=drive_link
"""

import pandas as pd # module yg digunakan untuk analisis data
import numpy as np # module yg digunakan untuk operasi matematika
import matplotlib.pyplot as plt # module yg digunakan untuk melakukan visualisasi data
import seaborn as sns # module yg digunakan untuk melakukan visualisasi data
import os # untuk melakukan operasi sistem di google colab
import tempfile
from google.colab import files

from google.colab import files
uploaded = files.upload()

movies_data = pd.read_csv('movies_metadata.csv')

movies_data.info()

movies_data['release_date']=pd.to_datetime(movies_data['release_date'])
movies_data['release_date']

"""FILTERING, SORTING, DUPLICATED, MISSING VALUE"""

# filter release movie

released_movies = movies_data[movies_data['status']=='Released']
released_movies

# filter release movie sorting highest vote

released_movies.sort_values('vote_count', ascending = False).head(10)

# cek duplicate

released_movies.duplicated().sum()

# menghapus duplicate

released_movies.drop_duplicates(inplace=True)

# missing value

released_movies.isna().sum()

# handling missing value dengan fillna()

released_movies['belongs_to_collection'] = released_movies['belongs_to_collection'].fillna('No Data')
released_movies['homepage'] = released_movies['homepage'].fillna('No Data')
released_movies['tagline'] = released_movies['tagline'].fillna('No Tagline')
released_movies['overview'] = released_movies['overview'].fillna('No Data')
released_movies['poster_path'] = released_movies['poster_path'].fillna('No Data')
released_movies['runtime'] = released_movies['runtime'].fillna('No Data')

released_movies.isna().sum()

# handling missing value dengan dropna()

released_movies.dropna(inplace= True)

# CONTOH KASUS 1 : Menampilkan 10 film yang telah dirilis dan diurutkan berdasarkan vote tertinggi

released_movies.sort_values('vote_count', ascending = False).head(10)

# CONTOH KASUS 2 : Menampilkan film yang berdurasi lebih dari 3 jam

released_movies_true = released_movies[~(released_movies['runtime']=='No Data')]
released_movies_true[released_movies_true['runtime'] > 60 * 3]

"""AGGREGATE"""

from google.colab import files
uploaded = files.upload()

rating = pd.read_csv('ratings_small.csv')

rating.head()

rating['timestamp'] = pd.to_datetime(rating['timestamp'],unit = 's')

rating.info()

rating.duplicated().sum()

movies_rating = rating.groupby('movieId')['rating'].agg(['mean','median','count'])
movies_rating

"""MERGING DATA"""

released_movies_rating = released_movies.merge(movies_rating, left_on="id", right_on="movieId",how= 'inner')
released_movies_rating

"""FEATURES ENGINEERING"""

# CONTOH KASUS

# 1. Membuat kolom profit = revenue - budget

released_movies_rating['profit'] = released_movies_rating['revenue'] - released_movies_rating['budget']

released_movies_rating[['original_title','profit','runtime']].sort_values('profit', ascending = False).head(10)

# CONTOH KASUS

# 2. Membuat kolom bahasa merupakan bahasa inggris atau bukan (is_English)

released_movies_rating['is_English'] = released_movies_rating['original_language'] == 'en'
released_movies_rating

# CONTOH KASUS

# 3. Membuat kolom panjang judul film (title_len)

released_movies_rating['title_len'] = released_movies_rating['original_title'].str.len()
released_movies_rating

# CONTOH KASUS

# 4. Membuat kolom runtime dari menit menjadi jam (0-n) (duration_hours)

released_movies_rating_true = released_movies_rating[~(released_movies_rating['runtime']=='No Data')]
released_movies_rating_true['duration_hours']=released_movies_rating_true['runtime']/60

# CONTOH KASUS

# 5. Membuat kolom favourite_movies dengan median rating > 3

released_movies_rating_true['favourite_movie'] = released_movies_rating_true['median'] > 3

# BONUS

released_movies_rating_true['year_release']=released_movies_rating_true['release_date'].dt.year

released_movies_rating_true

year_release = released_movies_rating_true.groupby('year_release').size()
year_release.sort_values(ascending = False)

"""EDA

Membuat pertanyaan :

1. Bagaimana distribusi dari film-film yang memiliki rating >3?
2. Bagaimana distribusi dari durasi film yg ada?
3. Apakah ada hubungan antara durasi dengan rating film yang didapatkan?
4. Bagaimana distribusi dari keuntungan (profit) yang dihasilkan oleh suatu film?
5. Apakah ada hubungan antara profit dan rating?

"""

# 1

movies_rating['favourite_movie'] = (movies_rating['median'] >= 3).astype('int')
movies_rating['favourite_movie'].value_counts().plot(kind = 'pie')
plt.title('Favourite Movies Distribution')
plt.show()

movies_data['profit'].describe()

movies_data['profit'].plot(kind = 'box')

import seaborn as sns

sns.boxplot(data = movies_data, x = 'profit')
plt.show()

sns.boxplot(data = movies_data, x = 'budget')
plt.show()

# 2

sns.boxplot(data = movies_data, x = 'runtime')
plt.show()

movies_data['runtime'].hist(bins=10)
plt.title('Duration Movies Distribution')
plt.show()

movies_data_rating = movies_data.merge(movies_rating, left_on="id", right_on="movieId",how= 'inner')
movies_data_rating

# 3

sns.scatterplot(data = movies_data_rating, x = 'median', y = 'runtime')
plt.title('Duration & Rating Relationship')
plt.show()

# 4

movies_data['profit'] = movies_data['revenue'] - movies_data['budget']
movies_data['profit'].hist(bins=30)
plt.title('Profit Movies Distribution')
plt.show()

movies_data['profit'].quantile(0.9)

"""Insight : Hanya ada 10% film yang mampu memberikan profit. Sisanya balik modal saja."""

# 5

sns.scatterplot(data = movies_data_rating, x = 'median', y = 'profit')
plt.title('Profit & Rating Relationship')
plt.show()

profitable_movies = movies_data_rating[movies_data_rating['profit'] > 0]

sns.scatterplot(data = profitable_movies, x = 'runtime', y = 'profit')
plt.title('Profit & Duration Relationship')
plt.show()

rating_movies = rating.groupby('movieId', as_index=False)['rating'].median()

sns.histplot(data = rating_movies, x = 'rating')
plt.show()

# 1

rating_movies['good_movies'] = rating_movies['rating'] >= 3
rating_movies['good_movies'].value_counts().plot(kind = 'pie')
plt.show()

rating_movies['good_movies'].value_counts(normalize=True)

new_df = profitable_movies.merge(rating_movies, left_on='id', right_on='movieId', how = 'inner')

sns.histplot(data = new_df, x = 'profit', hue = 'favourite_movie')
plt.show()

sns.scatterplot(data=new_df,
                x = 'runtime',
                y = 'profit',
                hue = 'favourite_movie')
plt.title('Profit VS Duration')
plt.show()

sns.scatterplot(data=new_df,
                x = 'rating',
                y = 'profit')
plt.title('Profit VS Rating')
plt.show()

"""untuk mendapat profit > 500jt, film harus mendapatkan rating min 3"""

new_df['genres'].head().apply(lambda x: eval(x)[0]['name'])

new_df['original_language'].value_counts().plot(kind='bar')

new_df['len_title'] = new_df['original_title'].str.len()
new_df['len_title'].hist(bins=100)

"""1.   Sebagian besar film memiliki panjang judul < 30 huruf
2.   Terdapat beberapa film yang memiliki judul yang cukup panjang, yaitu di atas 40 huruf

MACHINE LEARNING WORKFLOW
"""

new_df.select_dtypes(include='number')

def select_columns(df: pd.DataFrame, features: list, target: list ='rating'):
  X = df[features]
  y = (df[target] >= 3).astype('int')

  return X, y

from sklearn.model_selection import train_test_split

features = ['budget', 'popularity', 'runtime', 'vote_average']
X, y = select_columns(new_df, features = features)

y

"""Split Data"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)

X_train

from sklearn.preprocessing import MinMaxScaler

minmax = MinMaxScaler()

minmax.fit(X_train)

X_train_scaled = minmax.transform(X_train)
X_test_scaled = minmax.transform(X_test)

def scaling(df, scaler):
  scaled_array = scaler.transform(df)
  scaled_df = pd.DataFrame(scaled_array, columns = minmax.get_feature_names_out())

  return scaled_df

scaling(X_train, minmax)

scaling(X_test, minmax)

X_train_scaled = scaling(X_train, minmax)
X_test_scaled = scaling(X_test, minmax)

X_train_scaled

"""SUPERVISED MACHINE LEARNING"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier

knn = KNeighborsClassifier(n_neighbors=5)
dt = DecisionTreeClassifier()

knn.fit(X_train_scaled, y_train)
dt.fit(X_train_scaled, y_train)

y_train_pred_knn = knn.predict(X_train_scaled)
y_train_pred_dt = dt.predict(X_train_scaled)

y_test_pred_knn = knn.predict(X_test_scaled)
y_test_pred_dt = dt.predict(X_test_scaled)

pd.crosstab(y_test, y_test_pred_knn)

pd.crosstab(y_test, y_test_pred_dt)

"""MODEL EVALUATION"""

from sklearn.metrics import confusion_matrix, classification_report

confusion_matrix(y_test, y_test_pred_dt)

print(classification_report(y_test, y_test_pred_dt))

print(classification_report(y_test, y_test_pred_knn))

from sklearn.metrics import recall_score, precision_score, accuracy_score, f1_score

print('recall: ', recall_score(y_test, y_test_pred_knn))
print('precision: ', precision_score(y_test, y_test_pred_knn))
print('accuracy: ', accuracy_score(y_test, y_test_pred_knn))
print('f1 score: ', f1_score(y_test, y_test_pred_knn))

"""TUNING"""

# hyperparameter tuning

knn.get_params()

knn_params = KNeighborsClassifier(n_neighbors = 7, p=3, )

knn_params.fit(X_train, y_train)

print(classification_report(y_test, knn_params.predict(X_test)))

print(confusion_matrix(y_test, knn_params.predict(X_test)))

knn_params = {
    'n_neighbors' : [3,5,7,9],
    'p' : [1,2,3,4,5]
}

from sklearn.model_selection import GridSearchCV

knn_new = GridSearchCV(KNeighborsClassifier(),
                       knn_params)

knn_new.fit(X_train, y_train)

knn_new.best_params_

knn_best = knn_new.best_estimator_

knn_new.best_score_

# Threshold tuning

prob = knn_best.predict_proba(X_test)[:, 1]

(prob >= 0.5).astype('int')

knn_best.predict(X_test)

recall_ = []
precision_ = []
f1_score_ = []
accuracy_ = []

for i in range(1,11) :
  y_best_knn_proba = knn_best.predict_proba(X_test)[: ,1]
  recall_.append(recall_score(y_test, y_best_knn_proba >= i/10))
  precision_.append(precision_score(y_test, y_best_knn_proba >= i/10))
  accuracy_.append(accuracy_score(y_test, y_best_knn_proba >= i/10))
  f1_score_.append(f1_score(y_test, y_best_knn_proba >= i/10))

metrics = pd.DataFrame({'recall': recall_,
              'precision': precision_,
              'f1_score': f1_score_,
              'accuracy': accuracy_})

metrics['fpr'] = 1 - metrics['recall']

metrics.plot(x = 'fpr', y = 'recall')

from sklearn.metrics import roc_auc_score

roc_auc_score(y_test, knn_best.predict(X_test))

